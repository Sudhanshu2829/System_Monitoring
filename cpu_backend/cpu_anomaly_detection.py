# -*- coding: utf-8 -*-
"""CPU_anomaly_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QaGXecRrtJgFlHh8aZNQDFgwWLnrn-4Z
"""



import psutil
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer
from sklearn.linear_model import LinearRegression
import shap

def median_absolute_deviation(data):
    median = np.median(data)
    deviations = np.abs(data - median)
    return np.median(deviations)

def collect_metrics():
    """Collect observability metrics (CPU, Memory, Disk, etc.)"""
    cpu = psutil.cpu_percent(interval=1, percpu=True)
    memory = psutil.virtual_memory().percent
    disk = psutil.disk_usage('/').percent
    # Add more metrics as needed
    return {"CPU": cpu, "Memory": memory, "Disk": disk}

def preprocess_data(cpu_data):
    """Flatten the list of CPU data points"""
    flattened_cpu_data = [usage for sublist in cpu_data for usage in sublist]
    return flattened_cpu_data

def detect_anomalies(data, contamination):
    """Detect anomalies using Isolation Forest model"""
    # Initialize Isolation Forest model with the best contamination parameter
    model = IsolationForest(contamination=contamination, random_state=42)
    # Fit the model to the data and predict anomalies
    model.fit(data)
    anomalies = model.predict(data)
    return anomalies

def engineer_features(metrics):
    """Calculate the moving average of CPU usage over a window"""
    window_size = 5
    cpu_data = metrics["CPU"]
    moving_average_cpu = [np.mean(cpu_data[i:i+window_size]) for i in range(len(cpu_data) - window_size + 1)]
    # Reshape the 1D array to have two dimensions
    moving_average_cpu = np.array(moving_average_cpu).reshape(-1, 1)
    return moving_average_cpu

def custom_scoring_function(estimator, X, y_true):
    # Predict anomalies using the estimator
    y_pred = estimator.predict(X)
    # Convert anomaly labels (-1) to 1 and non-anomaly labels (1) to 0 for compatibility with precision and recall metrics
    y_true_binary = [1 if label == -1 else 0 for label in y_true]
    # Convert predictions (-1 for anomaly, 1 for normal) to binary predictions (1 for anomaly, 0 for normal)
    y_pred_binary = [1 if label == -1 else 0 for label in y_pred]
    # Compute F1-score
    f1 = f1_score(y_true_binary, y_pred_binary)
    return f1

def classify_anomalies(moving_average_cpu, anomalies):
    """Classify anomalies based on the magnitude of the anomaly"""
    anomaly_classes = []
    for i, anomaly in enumerate(anomalies):
        if anomaly == 1:
            anomaly_classes.append("No Anomaly")
        elif moving_average_cpu[i] < 10:
            anomaly_classes.append("Low Anomaly")
        elif moving_average_cpu[i] < 30:
            anomaly_classes.append("Medium Anomaly")
        else:
            anomaly_classes.append("High Anomaly")
    return anomaly_classes

def main():
    """Collect observability metrics, preprocess the data, detect anomalies, and classify them"""
    # Collect observability metrics
    metric_data = []
    for _ in range(60):  # Collect data for 10 minutes (adjust as needed)
        metric_data.append(collect_metrics())

    # Extract and preprocess CPU usage data
    cpu_data = [metric["CPU"] for metric in metric_data]
    flattened_cpu_data = preprocess_data(cpu_data)

    # Feature engineering: Calculate moving average of CPU usage
    moving_average_cpu = engineer_features({"CPU": flattened_cpu_data})

    # Hyperparameter tuning using GridSearchCV
    param_grid = {'contamination': [0.01, 0.02, 0.03, 0.04, 0.05]}
    model = IsolationForest(random_state=42)
    custom_scorer = make_scorer(custom_scoring_function)
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring=make_scorer(custom_scorer, greater_is_better=True))

    grid_search.fit(np.array(moving_average_cpu).reshape(-1, 1))

    best_contamination = grid_search.best_params_['contamination']
    print(f"Best contamination parameter: {best_contamination}")

    # True labels for anomalies
    anomalies = detect_anomalies(moving_average_cpu, contamination=best_contamination)
    y_true = [-1 if anomaly == 1 else 1 for anomaly in anomalies]

    # Detect anomalies with the tuned model
    anomalies = detect_anomalies(moving_average_cpu, contamination=best_contamination)

    # Train a linear surrogate model
    surrogate_model = LinearRegression()
    surrogate_model.fit(moving_average_cpu, anomalies)

    # SHAP (SHapley Additive exPlanations) for explaining anomalies
    explainer = shap.Explainer(surrogate_model, moving_average_cpu)
    shap_values = explainer.shap_values(moving_average_cpu)

    # Threshold Adaptation using Median Absolute Deviation (MAD)
    mad_threshold = 3 * median_absolute_deviation(moving_average_cpu)
    temporal_anomalies = [i for i, anomaly in enumerate(anomalies) if anomaly == -1]

    # Anomaly Aggregation over time
    aggregated_anomalies = [0] * len(moving_average_cpu)
    for i in temporal_anomalies:
        if moving_average_cpu[i] > mad_threshold:
            aggregated_anomalies[i] = 1

    # Classify anomalies based on magnitude
    anomaly_classes = classify_anomalies(moving_average_cpu, aggregated_anomalies)

    print("Anomalies detected using MAD threshold with severity classification:")
    for i, (anomaly, severity) in enumerate(zip(aggregated_anomalies, anomaly_classes)):
        if anomaly == 1:
            print(f"Anomaly detected at data point {i}: {moving_average_cpu[i]}% CPU usage (Severity: {severity})")

if __name__ == "__main__":
    main()